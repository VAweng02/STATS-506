---
title: "STATS 506 - Homework 3"
author: "Vincent Weng"
format:
  html:
    embed-resources: true
---

<h3>Problem 1 - Vision</h3>

Part (A) - Download the file VIX_D from this location, and determine how to read it into R. Then download the file DEMO_D from this location. Note that each page contains a link to a documentation file for that data set. Merge the two files to create a single data.frame, using the SEQN variable for merging. Keep only records which matched. Print out your total sample size, showing that it is now 6,980.
```{r}
library(haven)

vix_d <- read_xpt("/Users/vincentweng/Documents/STATS-506/HW3/VIX_D.XPT")
demo_d <- read_xpt("/Users/vincentweng/Documents/STATS-506/HW3/DEMO_D.XPT")

vision <- merge(vix_d, demo_d, by = "SEQN", all = FALSE)

nrow(vision)
```
The total sample size is now 6,980, as shown above.


Part (B) - Without fitting any models, estimate the proportion of respondents within each 10-year age bracket (e.g. 0-9, 10-19, 20-29, etc) who wear glasses/contact lenses for distance vision. Produce a nice table with the results.
```{r}
library(knitr)

vision$glasses <- ifelse(vision$VIQ220 == 9, NA, vision$VIQ220 - 1)
vision$age <- vision$RIDAGEYR
vision$agecat <- floor(vision$age / 10)

prop <- tapply(vision$glasses, vision$agecat, mean, na.rm = TRUE) * 100

prop_df <- data.frame(AgeBracket = c("10-19", "20-29", "30-39", "40-49", 
                                     "50-59", "60-69", "70-79", "80-89"),
                                      Proportion = round(prop, 1))
prop_df
```



Part(C) - Fit three logistic regression models predicting whether a respondent wears glasses/contact lenses for distance vision. Predictors:

1. age
2. age, race, gender
3. age, race, gender, Poverty Income ratio

Produce a table presenting the estimated odds ratios for the coefficients in each model, along with the sample size for the model, the pseudo-$R^2$, and AIC values.
```{r}
vision$age <- vision$RIDAGEYR
vision$gender <- vision$RIAGENDR
vision$race <- vision$RIDRETH1
vision$pov_inc_ratio <- vision$INDFMPIR

# model 1: age as predictor
model1 <- glm(glasses ~ age, data = vision, family = binomial)

# model 2: age, race, gender as predictors
model2 <- glm(glasses ~ age + race + gender, data = vision, family = binomial)

# model 3: age, race, gender, and Poverty Income ratio as predictors
model3 <- glm(glasses ~ age + race + gender + pov_inc_ratio, data = vision, family = binomial)

summary(model1)
summary(model2)
summary(model3)
```




Part(D) - From the third model from the previous part, test whether the odds of men and women being wears of glasess/contact lenses for distance vision differs. Test whether the proportion of wearers of glasses/contact lenses for distance vision differs between men and women. Include the results of the each test and their interpretation.
```{r}

```





<h3>Problem 2 - Sakila</h3>

```{r}
library(DBI)
library(RSQLite)

sakila <- dbConnect(RSQLite::SQLite(), "sakila_master.db")
```

Part(A) - What is the oldest movie (earliest release year) in the database? Answer this with a single SQL query.
```{r}
tables <- dbListTables(sakila)
print(tables)

columns <- dbListFields(sakila, "film")
print(columns)

dbGetQuery(sakila, "
  SELECT release_year, COUNT(title) AS movie_count
  FROM film
  WHERE release_year = (
      SELECT MIN(release_year) FROM film
  )
  GROUP BY release_year
")
```
2006 is the year with the oldest movie and 1000 movies were released that year.


Part(B) - What genre of movie is the least common in the data, and how many movies are of this genre?
```{r}
# load tables into R data.frame
category <- dbReadTable(sakila, "category")
film_category <- dbReadTable(sakila, "film_category")

# merge the two df based on category_id
merged_data <- merge(category, film_category, by = "category_id")

# count movies in each category
movie_counts <- table(merged_data$name)

# find category with the min count
min_count_category <- names(movie_counts)[which.min(movie_counts)]
min_count <- min(movie_counts)

cat("Category with least movies:", min_count_category, "\n")
cat("Number of movies in this category:", min_count, "\n")

# single SQL query
dbGetQuery(sakila, "
  SELECT c.name, COUNT(c.category_id)
  FROM category AS c
  JOIN film_category AS fc ON c.category_id = fc.category_id
  GROUP BY c.category_id
  ORDER by COUNT(c.category_id) ASC
  LIMIT 1
")
```
The lease common genrie in the data is Music and there are 51 movies in this genre.



Part(C) - Identify which country or countries have exactly 13 customers.
```{r}
# load tables into R data.frame
country <- dbReadTable(sakila, "country")
city <- dbReadTable(sakila, "city")
address <- dbReadTable(sakila, "address")
customer <- dbReadTable(sakila, "customer")

# joining tables together
merged_data <- merge(country, city, by.x = "country_id", by.y = "country_id")
merged_data <- merge(merged_data, address, by.x = "city_id", by.y = "city_id")
merged_data <- merge(merged_data, customer, by.x = "address_id", by.y = "address_id")

# calculate counts by country
country_counts <- table(merged_data$country)

# filter countries with 13 customers
print(country_counts[country_counts == 13])

# single SQL query
dbGetQuery(sakila, "
  SELECT co.country, count(co.country)
  FROM country AS co
  JOIN city AS ci ON co.country_id = ci.country_id
  JOIN address AS ad ON ci.city_id = ad.city_id
  JOIN customer as cu ON ad.address_id = cu.address_id
  GROUP BY co.country
  HAVING count(co.country) == 13
")
```
Argentina and Nigeria have exactly 13 customers.





<h3>Problem 3 - US Records</h3>

Part(A) - What proportion of email addresses are hosted at a domain with TLD “.com”? (in the email, “angrycat@freemail.org”, “freemail.org” is the domain, and “.org” is the TLD (top-level domain).)
```{r}
# read in data into R data.frame
us_records <- read.csv("us-500.csv")

# calculate count of emails with domain ".com"
count <- sum(grepl("\\.com$", us_records$email))

# dividing the above count by total to get proportion
print(count/nrow(us_records))
```


Part(B) - What proportion of email addresses have at least one non alphanumeric character in them? (Excluding the required “@” and “.” found in every email address.)
```{r}
# extract usernames and domains using regular expressions
usernames <- sub("@.*", "", us_records$email)   # remove everything after '@' to get username
domains <- sub(".*@", "", us_records$email)     # remove everything before '@' to get domain
domains <- sub("\\.[a-z]{2,3}$", "", domains)   # remove TLD (e.g., .com, .org)

# check non-alphanumeric chars in usernames and domains
username_non_alphanum <- grepl("[^a-zA-Z0-9]", usernames)
domain_non_alphanum<- grepl("[^a-zA-Z0-9]", domains)

# calculate proportion of emails with non-alphanumeric characters
print(mean(username_non_alphanum | domain_non_alphanum))
```
From the above, 50.6% of email addresses have at least one non alphanumeric character in them.


Part(C) - What are the top 5 most common area codes amongst all phone numbers? (The area code is the first three digits of a standard 10-digit telephone number.)
```{r}
# combine phone1 and phone2 columns
phone_numbers <- c(us_records$phone1, us_records$phone2)

# extract area codes
area_codes <- substr(phone_numbers, 1, 3)

# calculate counts of each area code
area_code_counts <- table(area_codes)

# sort by count and filter for top 5 most common area codes
top_five <- sort(area_code_counts, decreasing = TRUE)[1:5]

print(top_five)
```
The top 5 most common area codes are "973", "212", "215", "410", and "201".



Part(D) - Produce a histogram of the log of the apartment numbers for all addresses. (You may assume any number at the end of the an address is an apartment number.)
```{r}
# find indices of addresses that end with a number
indices <- grep("[0-9]+$", us_records$address)

# subset the addresses using the identified indices
apt <- us_records$address[indices]

# split addresses by spaces and extract the last element (assumed to be a number)
numbers <- sapply(strsplit(apt, " "), function(x) x[length(x)])

nums <- log(as.numeric(gsub("#", "", nums)))

hist(nums)
```



Part(E) - Benford’s law is an observation about the distribution of the leading digit of real numerical data. Examine whether the apartment numbers appear to follow Benford’s law. Do you think the apartment numbers would pass as real data?
```{r}
leading_digit_counts <- table(substr(nums, 1, 1))

# normalize the counts to get observed frequencies
observed_freq <- leading_digit_counts / sum(leading_digit_counts)

# calculate expected frequencies based on Benford's Law
benford_probs <- log10((1:9) + 1) - log10(1:9)
benford_freq <- benford_probs / sum(benford_probs)

# create a data frame for comparison
comparison_df <- data.frame(
  Digit = 1:9,
  Observed = as.numeric(observed_freq[1:9]),
  Expected = benford_freq
)

print(comparison_df)
```

From the above results, the observed distribution does not closely follow Benford's Law

