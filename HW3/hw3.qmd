---
title: "STATS 506 - Homework 3"
author: "Vincent Weng"
format:
  html:
    embed-resources: true
---

<h3>Problem 1 - Vision</h3>

Part (A) - Download the file VIX_D from this location, and determine how to read it into R. Then download the file DEMO_D from this location. Note that each page contains a link to a documentation file for that data set. Merge the two files to create a single data.frame, using the SEQN variable for merging. Keep only records which matched. Print out your total sample size, showing that it is now 6,980.
```{r}
library(haven)

# read in VIX_D.XPT
vix_d <- read_xpt("/Users/vincentweng/Documents/STATS-506/HW3/VIX_D.XPT")

# read in DEMO_D.XPT
demo_d <- read_xpt("/Users/vincentweng/Documents/STATS-506/HW3/DEMO_D.XPT")

# merge the two datasets
vision <- merge(vix_d, demo_d, by = "SEQN", all = FALSE)

nrow(vision) # total sample size
```
As shown above, the total sample size is now 6,980.


Part (B) - Without fitting any models, estimate the proportion of respondents within each 10-year age bracket (e.g. 0-9, 10-19, 20-29, etc) who wear glasses/contact lenses for distance vision. Produce a nice table with the results.
```{r}
library(knitr)

# create 'glasses' column: NA if VIQ220 is 9, otherwise VIQ220 - 1
vision$glasses <- ifelse(vision$VIQ220 == 9, NA, vision$VIQ220 - 1)

# assign 'RIDAGEYR' to a new column 'age'
vision$age <- vision$RIDAGEYR

# create new column 'agecat' with age/10 and rounded down
vision$agecat <- floor(vision$age / 10)

# calculate mean of 'glasses' for each agecat and getting the proportion
prop <- tapply(vision$glasses, vision$agecat, mean, na.rm = TRUE) * 100

# create new df 'prop_df' with age brackets and their corresponding proportions
prop_df <- data.frame(AgeBracket = c("10-19", "20-29", "30-39", "40-49", 
                                     "50-59", "60-69", "70-79", "80-89"),
                                      Proportion = round(prop, 1))
prop_df
```



Part(C) - Fit three logistic regression models predicting whether a respondent wears glasses/contact lenses for distance vision. Predictors:

1. age
2. age, race, gender
3. age, race, gender, Poverty Income ratio

Produce a table presenting the estimated odds ratios for the coefficients in each model, along with the sample size for the model, the pseudo-$R^2$, and AIC values.
```{r}
vision$age <- vision$RIDAGEYR  # assign 'RIDAGEYR' to 'age'
vision$gender <- vision$RIAGENDR  # assign 'RIAGENDR' to 'gender'
vision$race <- vision$RIDRETH1  # assign 'RIDRETH1' to 'race'
vision$pov_inc_ratio <- vision$INDFMPIR  # assign 'INDFMPIR' to 'pov_inc_ratio'

vision$female <- ifelse(vision$gender == 2, 1, 0) # binary female variable
vision$female[is.na(vision$female)] <- NA  # set missing values to NA

# convert race to a factor with specific labels for each level
vision$race <- factor(vision$race, levels = 1:5, 
                      labels = c("Mexican American", "Other Hispanic", 
                                 "Non-Hispanic White", "Non-Hispanic Black", 
                                 "Multi-racial"))

# model 1: age as predictor
model1 <- glm(glasses ~ age, data = vision, family = binomial)

# model 2: age, race, gender as predictors
model2 <- glm(glasses ~ age + race + gender, data = vision, family = binomial)

# model 3: age, race, gender, and Poverty Income ratio as predictors
model3 <- glm(glasses ~ age + race + gender + pov_inc_ratio, data = vision, family = binomial)


#' Extract Model Information
#' @description extracts necessary information from models produced above
#' @param num_rolls: model
extract_model_info <- function(model) {
  odds_ratios <- exp(coef(model)) # calculate odds ratios
  sample_size <- model$df.null + 1 # sample size
  pseudo_r2 <- 1 - model$deviance / model$null.deviance # pseudo-R^2 
  aic <- AIC(model) # aic value
  
  # create df with required information
  data.frame(
    term = names(odds_ratios),
    odds_Ratio = odds_ratios,
    sample_Size = sample_size,
    pseudo_R2 = pseudo_r2,
    AIC = aic
  )
}

# get model info for each model
model1_info <- extract_model_info(model1)
model2_info <- extract_model_info(model2)
model3_info <- extract_model_info(model3)

# add model identifiers and combine results
model_info_table <- rbind(
  cbind(Model = "Model 1", model1_info),
  cbind(Model = "Model 2", model2_info),
  cbind(Model = "Model 3", model3_info)
)

print(model_info_table)
```
The table is shown above. Furthermore, NAs were not removed, because in logistic regression models (glm), rows with NA in any predictor are automatically excluded from model fitting by default. Thus, we can keep the data clean while ensuring missing values don't affect the model fitting process.



Part(D) - From the third model from the previous part, test whether the odds of men and women being wears of glasses/contact lenses for distance vision differs. Test whether the proportion of wearers of glasses/contact lenses for distance vision differs between men and women. Include the results of the each test and their interpretation.
```{r}
summary_model3 <- summary(model3)

# Extract gender results
gender_results <- summary_model3$coefficients["gender", ]  # Get gender row
odds_ratio_gender <- exp(gender_results["Estimate"])  # Calculate odds ratio
se_gender <- gender_results["Std. Error"]  # Get standard error
z_gender <- gender_results["z value"]  # Get z-value
p_value_gender <- gender_results["Pr(>|z|)"]  # Get p-value

# print results
cat("Odds ratio for gender (female vs male):", odds_ratio_gender, "\n")
cat("Standard error:", se_gender, "\n")
cat("z-value:", z_gender, "\n")
cat("p-value:", p_value_gender, "\n")
```
The estimated odds ratio is 0.5967415 and the p-value is 1.96446e-21. This means that this is statistically significant in that the odds of men and women wearing glasses/contact lenses for distance vision do differ. Furthermore, the odds of women wearing glasses/contacts for distance vision is lower than the odds of men wearing glasses/contacts for distance vision, with statistical significance. 

```{r}
library(margins)

# Compute margins for 'female' (gender) variable in model3
margins_female <- margins(model3, variables = "gender")
summary(margins_female)

# Pairwise comparison for predictive margins by gender
margins_female_pw <- summary(margins(model3, variables = "gender", at = list(gender = c(0, 1)), pwcompare = TRUE))
margins_female_pw
```
The above results show that women has a significantly lower probability of wearing glasses/contact lenses for distance vision (by about 10.96%) compared to men, with very strong statistical significance (p < 0.0001).



<h3>Problem 2 - Sakila</h3>

```{r}
library(DBI)
library(RSQLite)

sakila <- dbConnect(RSQLite::SQLite(), "sakila_master.db")
```

Part(A) - What is the oldest movie (earliest release year) in the database? Answer this with a single SQL query.
```{r}
# single SQL query to get oldest movie year and number of movies in that year
dbGetQuery(sakila, "
  SELECT release_year, COUNT(title) AS movie_count
  FROM film
  WHERE release_year = (
      SELECT MIN(release_year) FROM film
  )
  GROUP BY release_year
")
```
2006 is the year with the oldest movie and 1000 movies were released that year.


Part(B) - What genre of movie is the least common in the data, and how many movies are of this genre?
```{r}
# query to get merged table of category and film_category
category_movie_counts <- dbGetQuery(sakila, "
  SELECT category.name, COUNT(film_category.film_id) AS movie_count
  FROM category
  JOIN film_category ON category.category_id = film_category.category_id
  GROUP BY category.name
")

# find category with the minimum count
min_count_category <- category_movie_counts$name[which.min(category_movie_counts$movie_count)]
min_count <- min(category_movie_counts$movie_count)

# print out results
cat("Category with least movies:", min_count_category, "\n")
cat("Number of movies in this category:", min_count, "\n")

# single SQL query to get least common movie genre
dbGetQuery(sakila, "
  SELECT c.name, COUNT(c.category_id)
  FROM category AS c
  JOIN film_category AS fc ON c.category_id = fc.category_id
  GROUP BY c.category_id
  ORDER by COUNT(c.category_id) ASC
  LIMIT 1
")
```
The least common genre in the data is Music and there are 51 movies in this genre.



Part(C) - Identify which country or countries have exactly 13 customers.
```{r}
# load each table into R df using SQL queries
country <- dbGetQuery(sakila, "SELECT * FROM country")
city <- dbGetQuery(sakila, "SELECT * FROM city")
address <- dbGetQuery(sakila, "SELECT * FROM address")
customer <- dbGetQuery(sakila, "SELECT * FROM customer")

# joining tables together
merged_data <- merge(country, city, by.x = "country_id", by.y = "country_id")
merged_data <- merge(merged_data, address, by.x = "city_id", by.y = "city_id")
merged_data <- merge(merged_data, customer, by.x = "address_id", by.y = "address_id")

# calculate counts by country
country_counts <- table(merged_data$country)

# filter countries with 13 customers
print(country_counts[country_counts == 13])

# single SQL query
dbGetQuery(sakila, "
  SELECT co.country, count(co.country)
  FROM country AS co
  JOIN city AS ci ON co.country_id = ci.country_id
  JOIN address AS ad ON ci.city_id = ad.city_id
  JOIN customer as cu ON ad.address_id = cu.address_id
  GROUP BY co.country
  HAVING count(co.country) == 13
")
```
Argentina and Nigeria have exactly 13 customers.



<h3>Problem 3 - US Records</h3>

Part(A) - What proportion of email addresses are hosted at a domain with TLD “.com”? (in the email, “angrycat@freemail.org”, “freemail.org” is the domain, and “.org” is the TLD (top-level domain).)
```{r}
# read in data into R data.frame
us_records <- read.csv("us-500.csv")

# calculate count of emails with domain ".com"
count <- sum(grepl("\\.com$", us_records$email))

# dividing the above count by total to get proportion
print(count/nrow(us_records))
```
From the above, 73.2% of email addresses are hosted at a domain with TLD “.com”.


Part(B) - What proportion of email addresses have at least one non alphanumeric character in them? (Excluding the required “@” and “.” found in every email address.)
```{r}
# extract usernames and domains using regular expressions
usernames <- sub("@.*", "", us_records$email)   # remove everything after '@' to get username
domains <- sub(".*@", "", us_records$email)     # remove everything before '@' to get domain
domains <- sub("\\.[a-z]{2,3}$", "", domains)   # remove TLD (e.g., .com, .org)

# check non-alphanumeric chars in usernames and domains
username_non_alphanum <- grepl("[^a-zA-Z0-9]", usernames)
domain_non_alphanum<- grepl("[^a-zA-Z0-9]", domains)

# calculate proportion of emails with non-alphanumeric characters
print(mean(username_non_alphanum | domain_non_alphanum))
```
From the above, 50.6% of email addresses have at least one non alphanumeric character in them.


Part(C) - What are the top 5 most common area codes amongst all phone numbers? (The area code is the first three digits of a standard 10-digit telephone number.)
```{r}
# combine phone1 and phone2 columns
phone_numbers <- c(us_records$phone1, us_records$phone2)

# extract area codes
area_codes <- substr(phone_numbers, 1, 3)

# calculate counts of each area code
area_code_counts <- table(area_codes)

# sort by count and filter for top 5 most common area codes
top_five <- sort(area_code_counts, decreasing = TRUE)[1:5]

print(top_five)
```
The top 5 most common area codes are "973", "212", "215", "410", and "201".



Part(D) - Produce a histogram of the log of the apartment numbers for all addresses. (You may assume any number at the end of the an address is an apartment number.)
```{r}
# find indices of addresses that end with a number
indices <- grep("[0-9]+$", us_records$address)

# subset the addresses using the identified indices
apt <- us_records$address[indices]

# split addresses by spaces and extract the last element (assumed to be a number)
nums <- sapply(strsplit(apt, " "), function(x) x[length(x)])

# convert apt numbers to logarithmic 
nums <- log(as.numeric(gsub("#", "", nums)))

# produce histogram
hist(nums)
```



Part(E) - Benford’s law is an observation about the distribution of the leading digit of real numerical data. Examine whether the apartment numbers appear to follow Benford’s law. Do you think the apartment numbers would pass as real data?
```{r}
leading_digit_counts <- table(substr(nums, 1, 1))

# normalize the counts to get observed frequencies
observed_freq <- leading_digit_counts / sum(leading_digit_counts)

# calculate expected frequencies based on Benford's Law
benford_probs <- log10((1:9) + 1) - log10(1:9)
benford_freq <- benford_probs / sum(benford_probs)

# create a data frame for comparison
comparison_df <- data.frame(
  Digit = 1:9,
  Observed = as.numeric(observed_freq[1:9]),
  Expected = benford_freq
)

print(comparison_df)
```

From the above results, the observed distribution does not closely follow Benford's Law and the data does not appear real.

